{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Business Problem Overview\n",
    "\n",
    "In the telecom industry, customers are able to choose from multiple service providers and actively switch from one operator to another. In this highly competitive market, the telecommunications industry experiences an average of 15-25% annual churn rate. Given the fact that it costs 5-10 times more to acquire a new customer than to retain an existing one, customer retention has now become even more important than customer acquisition.\n",
    "\n",
    " \n",
    "\n",
    "For many incumbent operators, retaining high profitable customers is the number one business goal.\n",
    "\n",
    " \n",
    "\n",
    "To reduce customer churn, telecom companies need to predict which customers are at high risk of churn.\n",
    "\n",
    " \n",
    "\n",
    "In this project, you will analyse customer-level data of a leading telecom firm, build predictive models to identify customers at high risk of churn and identify the main indicators of churn.\n",
    "\n",
    " \n",
    "## Understanding and Defining Churn\n",
    "\n",
    "There are two main models of payment in the telecom industry - postpaid (customers pay a monthly/annual bill after using the services) and prepaid (customers pay/recharge with a certain amount in advance and then use the services).\n",
    "\n",
    "In the postpaid model, when customers want to switch to another operator, they usually inform the existing operator to terminate the services, and you directly know that this is an instance of churn.\n",
    "\n",
    "However, in the prepaid model, customers who want to switch to another network can simply stop using the services without any notice, and it is hard to know whether someone has actually churned or is simply not using the services temporarily (e.g. someone may be on a trip abroad for a month or two and then intend to resume using the services again).\n",
    "\n",
    "Thus, churn prediction is usually more critical (and non-trivial) for prepaid customers, and the term ‘churn’ should be defined carefully.  Also, prepaid is the most common model in India and southeast Asia, while postpaid is more common in Europe in North America.\n",
    "\n",
    "This project is based on the Indian and Southeast Asian market.\n",
    "\n",
    "## Definitions of Churn\n",
    "\n",
    "There are various ways to define churn, such as:\n",
    "\n",
    "#### Revenue-based churn: \n",
    "Customers who have not utilised any revenue-generating facilities such as mobile internet, outgoing calls, SMS etc. over a given period of time. One could also use aggregate metrics such as _**‘customers who have generated less than INR 4 per month in total/average/median revenue’**_.\n",
    "\n",
    "The main shortcoming of this definition is that there are customers who only receive calls/SMSes from their wage-earning counterparts, i.e. they don’t generate revenue but use the services. For example, many users in rural areas only receive calls from their wage-earning siblings in urban areas.\n",
    "\n",
    "#### Usage-based churn: \n",
    "Customers **who have not done any usage, either incoming or outgoing - in terms of calls, internet etc. over a period of time**.\n",
    "\n",
    "    A potential shortcoming of this definition is that when the customer has stopped using the services for a while, it may be too late to take any corrective actions to retain them. For e.g., if you define churn based on a ‘two-months zero usage’ period, predicting churn could be useless since by that time the customer would have already switched to another operator.\n",
    "\n",
    "In this project, you will use the **usage-based definition to define _churn_ **.\n",
    "\n",
    " \n",
    "### High-value Churn\n",
    "\n",
    "In the Indian and the southeast Asian market,\n",
    "\n",
    "    approximately 80% of revenue comes from the top 20% customers (called high-value customers). \n",
    "  \n",
    "Thus, if we can reduce churn of the high-value customers, we will be able to reduce significant revenue leakage.\n",
    "\n",
    "In this project, you will define high-value customers based on a certain metric (mentioned later below) and predict churn only on high-value customers.\n",
    "\n",
    " \n",
    "### Understanding the Business Objective and the Data\n",
    "\n",
    "The dataset contains customer-level information for a span of four consecutive months - June, July, August and September. The months are encoded as 6, 7, 8 and 9, respectively. \n",
    "\n",
    "\n",
    "The business objective is to predict the churn in the last (i.e. the ninth) month using the data (features) from the first three months. To do this task well, understanding the typical customer behaviour during churn will be helpful.\n",
    "\n",
    " \n",
    "### Understanding Customer Behaviour During Churn\n",
    "\n",
    "Customers usually do not decide to switch to another competitor instantly, but rather over a period of time (this is especially applicable to high-value customers). In churn prediction, we assume that there are three phases of customer lifecycle :\n",
    "\n",
    "    The ‘good’ phase: In this phase, the customer is happy with the service and behaves as usual.\n",
    "\n",
    "    The ‘action’ phase: The customer experience starts to sore in this phase, for e.g. he/she gets a compelling offer from a  competitor, faces unjust charges, becomes unhappy with service quality etc. In this phase, the customer usually shows different behaviour than the ‘good’ months. Also, it is crucial to identify high-churn-risk customers in this phase, since some corrective actions can be taken at this point (such as matching the competitor’s offer/improving the service quality etc.)\n",
    "\n",
    "    The ‘churn’ phase: In this phase, the customer is said to have churned. You define churn based on this phase. Also, it is important to note that at the time of prediction (i.e. the action months), this data is not available to you for prediction. Thus, after tagging churn as 1/0 based on this phase, you discard all data corresponding to this phase.\n",
    "\n",
    " \n",
    "\n",
    "In this case, since you are working over a four-month window, the first two months are the ‘good’ phase, the third month is the ‘action’ phase, while the fourth month is the ‘churn’ phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions \n",
    "The space for user defined functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_triangular(corr):\n",
    "    mask = np.zeros_like(corr, dtype=np.bool)\n",
    "    mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "    # Set up the matplotlib figure\n",
    "    f, ax = plt.subplots(figsize=(11, 9))\n",
    "\n",
    "    # Generate a custom diverging colormap\n",
    "    cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "    # Draw the heatmap with the mask and correct aspect ratio\n",
    "    sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, annot=True, center=0,\n",
    "                square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_multiple_distributions(data, targets):\n",
    "    scale = StandardScaler(copy=False)\n",
    "    scaled_df = pd.DataFrame(scale.fit(data[targets]).transform(data[targets]), columns=targets)\n",
    "    for target in targets:\n",
    "        sns.distplot(scaled_df.loc[:, target], hist=False, rug=False, label=target)\n",
    "    return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import powerlaw\n",
    "def power_law_analysis(data, targets):\n",
    "    for target in targets:\n",
    "        results = powerlaw.Fit(data.loc[:, target])\n",
    "        print(results.power_law.alpha)\n",
    "        print(results.power_law.xmin)\n",
    "        R, p = results.distribution_compare('power_law', 'lognormal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ## 1. Load the data\n",
    "Load the churn data for the telecom provider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "churn_data = pd.read_csv('telecom_churn_data.csv')\n",
    "\n",
    "total_records = churn_data.shape[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Missing value treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count_rech_2g_6             74.85\n",
       "date_of_last_rech_data_6    74.85\n",
       "count_rech_3g_6             74.85\n",
       "av_rech_amt_data_6          74.85\n",
       "max_rech_data_6             74.85\n",
       "total_rech_data_6           74.85\n",
       "arpu_3g_6                   74.85\n",
       "arpu_2g_6                   74.85\n",
       "night_pck_user_6            74.85\n",
       "fb_user_6                   74.85\n",
       "arpu_3g_7                   74.43\n",
       "count_rech_2g_7             74.43\n",
       "fb_user_7                   74.43\n",
       "count_rech_3g_7             74.43\n",
       "arpu_2g_7                   74.43\n",
       "av_rech_amt_data_7          74.43\n",
       "max_rech_data_7             74.43\n",
       "night_pck_user_7            74.43\n",
       "total_rech_data_7           74.43\n",
       "date_of_last_rech_data_7    74.43\n",
       "night_pck_user_9            74.08\n",
       "date_of_last_rech_data_9    74.08\n",
       "fb_user_9                   74.08\n",
       "arpu_2g_9                   74.08\n",
       "max_rech_data_9             74.08\n",
       "arpu_3g_9                   74.08\n",
       "total_rech_data_9           74.08\n",
       "av_rech_amt_data_9          74.08\n",
       "count_rech_3g_9             74.08\n",
       "count_rech_2g_9             74.08\n",
       "fb_user_8                   73.66\n",
       "av_rech_amt_data_8          73.66\n",
       "count_rech_3g_8             73.66\n",
       "count_rech_2g_8             73.66\n",
       "date_of_last_rech_data_8    73.66\n",
       "total_rech_data_8           73.66\n",
       "max_rech_data_8             73.66\n",
       "arpu_3g_8                   73.66\n",
       "arpu_2g_8                   73.66\n",
       "night_pck_user_8            73.66\n",
       "dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_value_percentage = round(100 * churn_data.isnull().sum().sort_values(ascending=False) / total_records, 2)\n",
    "missing_value_percentage[missing_value_percentage > 50.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "non_kpis_to_be_imputed =['max_rech_data_6', 'max_rech_data_7',  'max_rech_data_8', 'max_rech_data_9']\n",
    "#This valuses doesnpt drive the KPI will replace by zero as per instruction\n",
    "churn_data[non_kpis_to_be_imputed] = churn_data[non_kpis_to_be_imputed].fillna(0, axis=1)\n",
    "\n",
    "#impute the categorical variable NaN with -1\n",
    "cat_variables_to_be_imputed = ['night_pck_user_6', 'night_pck_user_7',\n",
    "                               'night_pck_user_8', 'night_pck_user_9',\n",
    "                               'fb_user_6', 'fb_user_7',\n",
    "                               'fb_user_8', 'fb_user_9']\n",
    "churn_data[cat_variables_to_be_imputed] = churn_data[cat_variables_to_be_imputed].fillna(-1, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#date dependant missing values\n",
    "kpis_imputed_wrt_date = dict( date_of_last_rech_data_6 = ['total_rech_data_6', \n",
    "                                                          'count_rech_2g_6', \n",
    "                                                          'count_rech_3g_6', 'av_rech_amt_data_6' ],\n",
    "                             date_of_last_rech_data_7 = ['total_rech_data_7', \n",
    "                                                          'count_rech_2g_7', \n",
    "                                                          'count_rech_3g_7', 'av_rech_amt_data_7' ], \n",
    "                             date_of_last_rech_data_8 = ['total_rech_data_8', \n",
    "                                                          'count_rech_2g_8', \n",
    "                                                          'count_rech_3g_8', 'av_rech_amt_data_8'],\n",
    "                             date_of_last_rech_data_9 = ['total_rech_data_9', \n",
    "                                                          'count_rech_2g_9', \n",
    "                                                          'count_rech_3g_9', 'av_rech_amt_data_9'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#churn_data_clone = churn_data.copy()\n",
    "#Replace the date related values if date is present 1 else 0\n",
    "for date, fields in kpis_imputed_wrt_date.items():\n",
    "    churn_data.loc[churn_data[date].isnull(), fields] = churn_data.loc[churn_data[date].isnull(), fields].fillna(0)\n",
    "    churn_data.loc[churn_data[date].notnull(), fields] = churn_data.loc[churn_data[date].notnull(), fields].fillna(1)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Now we can drop the date related data\n",
    "date_cols = [col for col in churn_data.columns if 'date' in col.lower() ]\n",
    "churn_data.drop(labels=date_cols, axis=1,  inplace=True, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "unique_val_cols=churn_data.nunique(dropna=False).sort_values()\n",
    "single_value_cols = unique_val_cols[unique_val_cols < 3].index.values\n",
    "#Drop the columns with single value or null since this is not going to add any information for training\n",
    "churn_data.drop(labels=single_value_cols, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[d for val in churn_data.columns if 'date'in val.lower()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Possible categorical variables \n",
      " ['night_pck_user_7' 'night_pck_user_9' 'fb_user_9' 'fb_user_8' 'fb_user_7'\n",
      " 'fb_user_6' 'night_pck_user_6' 'night_pck_user_8' 'monthly_2g_6'\n",
      " 'monthly_2g_9' 'monthly_2g_7' 'monthly_2g_8' 'monthly_3g_9'\n",
      " 'monthly_3g_6' 'monthly_3g_8' 'monthly_3g_7' 'sachet_3g_6'\n",
      " 'count_rech_3g_6' 'count_rech_3g_9' 'sachet_3g_7' 'sachet_3g_9'\n",
      " 'count_rech_3g_7' 'count_rech_3g_8' 'sachet_3g_8' 'count_rech_2g_6'\n",
      " 'count_rech_2g_9' 'sachet_2g_9' 'sachet_2g_6' 'sachet_2g_8'\n",
      " 'count_rech_2g_8']\n",
      "Final Categorical variables \n",
      " ['night_pck_user_7', 'night_pck_user_9', 'fb_user_9', 'fb_user_8', 'fb_user_7', 'fb_user_6', 'night_pck_user_6', 'night_pck_user_8', 'monthly_2g_6', 'monthly_2g_9', 'monthly_2g_7', 'monthly_2g_8', 'monthly_3g_9', 'monthly_3g_6', 'monthly_3g_8', 'monthly_3g_7', 'sachet_3g_6', 'sachet_3g_7', 'sachet_3g_9', 'sachet_3g_8', 'sachet_2g_9', 'sachet_2g_6', 'sachet_2g_8']\n"
     ]
    }
   ],
   "source": [
    "#Let's check the other categorical data \n",
    "unique_val_cols=churn_data.nunique().sort_values()\n",
    "#Get the features with atmost 35 unique values. 35 is just a jueristics\n",
    "possible_cat_vars = unique_val_cols[unique_val_cols < 35].index.values\n",
    "print(\"Possible categorical variables \\n\", possible_cat_vars)\n",
    "#In Consulation with data dictionary the excluding the count_* features since they are quantile in nature\n",
    "categorical_vars = [var for var in possible_cat_vars if 'count_' not in var]\n",
    "print(\"Final Categorical variables \\n\", categorical_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The categorical variable identified:\n",
    "            1. 'night_pck_user_7' \n",
    "            2. 'night_pck_user_9'\n",
    "            3. 'fb_user_9'\n",
    "            4. 'fb_user_8' \n",
    "            5. 'fb_user_7' \n",
    "            6. 'fb_user_6', \n",
    "            7. 'night_pck_user_6', \n",
    "            8.'night_pck_user_8', \n",
    "            9. 'monthly_2g_6', \n",
    "            10.'monthly_2g_9', \n",
    "            11.'monthly_2g_7', \n",
    "            12.'monthly_2g_8',\n",
    "            13.'monthly_3g_9',\n",
    "            14 'monthly_3g_6',\n",
    "            15 'monthly_3g_8',\n",
    "            16 'monthly_3g_7',\n",
    "            17 'sachet_3g_6',\n",
    "            18 'sachet_3g_7', \n",
    "            19.'sachet_3g_9', \n",
    "            20 'sachet_3g_8', \n",
    "            21 'sachet_2g_9', \n",
    "            22.'sachet_2g_6', \n",
    "            23 'sachet_2g_8' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#churn_data.loc[:, categorical_vars].fillna(churn_data)\n",
    "#Replace the rest of missing data with mediam\n",
    "churn_data_clone = churn_data.copy()\n",
    "churn_data.fillna(value=churn_data.median(), inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number records with missing values : 0\n"
     ]
    }
   ],
   "source": [
    "missing_values = churn_data.isnull().sum()\n",
    "print(\"The number records with missing values :\", missing_values[missing_values!=0].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derived features\n",
    "Now that dealt with the missing data now we can look into derived features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "hv_kpis = ['total_rech_amt_6', 'total_rech_amt_7', \n",
    "           'total_rech_data_6', 'total_rech_data_7', \n",
    "           'av_rech_amt_data_6', 'av_rech_amt_data_7']\n",
    "churn_data['hv_index'] = churn_data[hv_kpis].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "hv_filtered = False\n",
    "if not hv_filtered :\n",
    "    churn_data = churn_data[churn_data.hv_index >= churn_data.hv_index.quantile(0.7)]\n",
    "    hv_filtered = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall churn rate 8.19%\n"
     ]
    }
   ],
   "source": [
    "churn_indicator = ['total_ic_mou_9', 'total_og_mou_9', 'vol_3g_mb_9', 'vol_2g_mb_9']\n",
    "#churn_data.loc[:, 'churned'] = churn_data.loc[:, churn_indicator].su\n",
    "churn_data['churn'] = churn_data.loc[:, churn_indicator].sum(axis=1).apply(lambda x: 1 if x == 0 else 0)\n",
    "print('Overall churn rate {}%'.format(round(100*churn_data['churn'].sum()/churn_data['churn'].shape[0], 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Outlier treatment\n",
    "Treating outlier values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1955"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "churn_data['total_arpu'] = churn_data.filter(regex='arpu_([6-9]+)').sum(axis=1)\n",
    "sum([len(churn_data[churn_data[var] < 0][[var, 'churn', 'total_arpu']]) for var in churn_data.filter(regex='arpu_')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "month_wise_data = dict()\n",
    "month_wise_data['jun'] = churn_data.filter(regex='(_6$|^jun_)')\n",
    "month_wise_data['jul'] = churn_data.filter(regex='(_7$|^jul_)')\n",
    "month_wise_data['aug'] = churn_data.filter(regex='(_8$|^aug_)')\n",
    "month_wise_data['sep'] = churn_data.filter(regex='(_9$|^sep_)')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "business_feature_class = ['std', 'ic', 'og', 't2t', 't2m', 't2o', \n",
    "                          't2f', 't2c', 'arpu', 'mou', 'aon', 'onnet', \n",
    "                          'offnet', 'roam', 'spl', 'isd', 'rech', 'num', \n",
    "                          'amt', 'max', 'data', '3g', 'av', 'vol', \n",
    "                          '2g', 'pck', 'night', 'monthly', 'sachet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "business_features = ['arpu_(6|7|8|9)', 'mou', 'amt', 'vol', 'vbc']\n",
    "length = len(business_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: MacOSX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/scipy/stats/stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n"
     ]
    }
   ],
   "source": [
    "#for mon, mon_data in month_wise_data.items():\n",
    "%matplotlib\n",
    "\n",
    "for feature in business_features:\n",
    "    plt.figure(figsize=(10,10))\n",
    "    feature_df = churn_data.filter(regex=feature)\n",
    "    plot_multiple_distributions(feature_df, feature_df.columns.values)\n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for feature in business_features:\n",
    "    feature_df = churn_data.filter(regex=feature)\n",
    "    print(feature_df.describe())\n",
    "    print('\\n', feature_df.median(), '\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
